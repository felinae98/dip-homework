来自视频序列的准确的3d感知是
计算机视觉和机器人技术的核心科目，因为它形成
后续场景分析的基础。但在实践中，
在线要求经常严重限制可用的相机
分辨率，因此也重建准确性。此外，实时系统通常依赖于严重的并行性，这可能会阻止移动设备或驾驶辅助系统中的应用，尤其是在无法使用FPGA的情况下。本文提出了一种从高分辨率立体声序列实时构建三维地图的新方法。受近期立体匹配进展的启发，我们提出了一种解析特征匹配器，以及一种高效且稳健的视觉测距算法。我们的重建管道将两种技术与高效的立体匹配相结合，并采用多视图链接方案生成一致的三维点云。在我们的实验中，我们表明所提出的测距方法达到了最先进的精度。包括特征匹配，我们算法的视觉测距部分以每秒25帧的速度运行，同时 - 我们以3-4 fps的速度获得新的深度图，足以进行在线3D重建。

如今，激光扫描仪仍广泛应用于机器人和自动驾驶汽车，主要是因为它们直接提供实时三维测量。然而，与传统相机系统相比，3d激光扫描仪通常更昂贵并且更难以无缝地集成到现有硬件设计（例如，汽车或火车）中。而且，它们容易干扰与它们基于主动感测原理相同类型的其他传感器。而且，它们的垂直分辨率是有限的（例如，Velodyne HDL-64E中的64个激光束）。诸如基于外观的物体检测和跟踪的经典计算机视觉技术受到反射率测量中的大量噪声的阻碍。受这些事实以及高分辨率视频传感器紧急可用性的推动，本文提出了一种新颖的系统，可以完全从立体序列中实现静态场景的精确三维重建1。据我们所知，我们的第一个系统能够在一个CPU上在线处理大约一百万像素分辨率的图像。我们的贡献有三个方面：首先，我们展示了几千个特征匹配的实时场景流计算。其次，提出了一种简单但强大的视觉测距算法，与现有技术相比，该算法达到了显着的加速。最后，使用获得的自我运动，我们以较低的帧速率集成来自LIBELAS [12]的密集立体测量和来自以下网站的1源代码：www.cvlibs.net

由于基于视频的三维重建是计算机视觉和机器人技术的核心课题，因此存在大量相关工作，简要概述如下：同时定位和制图（SLAM）[6]，[19]，[7]， [26]，[5]是移动机器人逐步建立其环境的一致地图并同时使用该地图计算其自身位置的过程。然而，出于计算原因，大多数提出的方法只能实时处理非常稀疏的地标集，而在这里我们感兴趣的是
密集映射解决方案随着Hoiem等人的开创性工作。 [14]基于学习的单眼图像几何估计方法已经复苏[23]，[13]。这些方法通常将图像分割成超像素，并且基于局部外观以及全局约束，推断出每个片段的最可能的3d配置。尽管最近已经证明了令人印象深刻的结果[13]，但这些方法仍然太不准确和错误，无法直接支持移动导航或自动驾驶等应用。 Koch [17]，Pollefeys [22]，Seitz [24]等人已经证明了未校准图像集合的三维重建。使用经典的Motion-from-Motion（SfM）技术。在[2]，[9]，[10]中已经证明了城市重建的扩展。最近，像Flickr这样的照片共享平台的可用性导致了对像罗马这样大的城市进行建模的努力[1]，[8]。然而，为了获得精确的半密集重建，采用了强大的多视图立体方案，即使在小图像集合上，也可以在大量使用并行处理设备的同时容易地花费几个小时。此外，大多数提出的方法需要多个冗余视点，而我们的应用目标是一个连续移动的移动平台，其中只能在短时间内观察对象。

在[3] Badino等人。 引入Stixel World作为中等水平表示，以减少传入传感器信息的数量。 他们观察到车辆前方的自由空间通常受到具有垂直表面的物体的限制，并且由相邻的固定宽度的矩形棒表示，随着时间的推移被跟踪[21]。 另一种经常使用的中级表示是占用网格[15]，[18]，它将3d世界离散化为二进制2d单元。 尽管在许多应用中很有用，但这些类型的抽象还不够详细，无法表示路边石或悬垂物体，如树木，标志或交通信号灯。 或者，可以使用3d体素网格。 然而，在不放弃分辨率的情况下，计算复杂性显着增加 相反，在本文中，我们感兴趣的是尽可能详细地表示感知信息，但不会失去实时性能。

我们的三维重建管道包括四个阶段：稀疏特征匹配，运动估计，密集立体匹配和三维重建。 我们假设CPU的两个核心可用，这样两个线程可以并行工作：如图2所示，第一个工作线程以25 fps执行特征匹配和运动估计，而第二个线程执行密集立体匹配和3d 重建速度为3到4 fps。 正如我们在实验中所示，这足以用于静态场景的在线三维重建。 在下文中，我们将假设校准的立体声设置和校正的输入图像，因为这代表标准情况并简化了计算。

我们的视觉测距算法的输入是在四个图像之间匹配的特征，即两个连续帧的左图像和右图像。为了找到稳定的特征位置，我们首先使用5 * 5斑点和角落掩模对输入图像进行滤波，如图3所示。接下来，我们对滤波采用非最大和非最小抑制[20]图像，导致属于四个类别之一的特征候选者（即，blob max，blob min，corner max，corner min）。为了减少计算工作量，我们只匹配这些类中的功能。与从无序图像集合中重建的方法相比，这里我们假设一个平滑的摄像机轨迹，取代计算强烈的旋转和尺度不变的特征描述符，如SURF [4]。给定两个特征点，我们简单地通过使用绝对差之和（SAD）误差度量来比较水平和垂直Sobel滤波器响应的11 * 11块窗口。为了加速匹配，我们将Sobel响应量化为8位，并在稀疏的16个位置集合上求和（见图3（c）），而不是在整个块窗口上求和。由于可以使用单个SSE指令有效地计算16字节的SAD，因此我们只需要两次调用（用于水平+垂直Sobel响应）以评估此错误度量。我们的运动估计机制期望在左图像和右图像以及两个连续帧之间匹配特征。这是通过匹配“圆圈”中的特征来实现的：从当前左图像中的所有候选特征开始，我们在M * M搜索窗口中找到前一个左图像中的最佳匹配，在前一个右图像中接下来，当前右图像再次在当前左图像中。如果最后一个要素与第一个要素重合，则会接受“圆圈匹配”。当在左图像和右图像之间进行匹配时，我们还使用1像素的误差容限来利用对极约束。通过将邻域关系建立为当前左图像中的特征位置上的2d Delaunay三角剖分[25]的边缘来消除零星异常值。我们仅保留由至少两个相邻匹配支持的匹配，其中匹配支持另一个匹配，如果其差异和流量差异分别落入某个阈值disp或flow内。如果需要，可以采用通过抛物线拟合的子像素细化来进一步改进特征定位。即使我们的实现非常有效，建立数千到数万个对应关系仍然需要几秒钟的时间，因此对于在线应用程序来说它太慢了。通过转移已经在以前的立体匹配工作中使用的想法[12]，可以进一步显着加速：在第一轮中，我们仅匹配所有特征的子集，由非最大值抑制（NMS）使用更大的NMS邻域找到大小（因子3）。由于此子集比完整功能集小得多，因此匹配非常快。接下来，我们将当前左图像中的每个特征分配给等间距网格的50 * 50像素区域。给定所有稀疏特征匹配，我们计算每个bin的最小和最大位移。这些统计数据用于局部缩小最终搜索空间，同时实现更快的匹配和更多的匹配，如实验部分所示。图4示出了使用我们的方法的特征匹配和跟踪结果。

